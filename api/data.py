
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: docs/api_data.ipynb

from pathlib import Path
import sys
sys.path.append(Path.cwd().parent.as_posix())

from data.scraper import *
from data.dataset import Dataset
from data.sampler import Sampler
from data.loader import DataLoader
from data.bunch import DataBunch

def normalize(x, m, s): return (x-m)/s

def data_processor(bs, url ="MNIST_URL"):
    x_train,y_train,x_valid,y_valid = get_data(url)
    c = y_train.max().item()+1
    train_mean,train_std = x_train.mean(),x_train.std()
    x_train = normalize(x_train, train_mean, train_std)
    # NB: Use training, not validation mean for validation set
    x_valid = normalize(x_valid, train_mean, train_std)

    train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)

    train_samp = Sampler(train_ds, bs, shuffle=True)
    valid_samp = Sampler(valid_ds, bs*2, shuffle=False)

    train_dl = DataLoader(train_ds, sampler=train_samp)
    valid_dl = DataLoader(valid_ds, sampler=valid_samp)
    return train_dl, valid_dl, c

class data():
    @classmethod
    def avaliable_urls(self):
        for key in urls:
            print(key, urls[key])
    @classmethod
    def bunch(self, *args):
        return DataBunch(*data_processor(*args))