
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: docs/nn_softmaxCrossEntropy.ipynb

import sys
from pathlib import Path
sys.path.append(Path.cwd().parent.as_posix())
from nn.neural import Neural

import torch

class SoftmaxCrossEntropy(Neural):

    def __repr__(self): return "Softmax()" +" & "+ "Cross_entropy_loss()"

    def softmax_forward(self, inp):
        m = inp.max(-1)[0]
        exps = (inp-m[:, None]).exp()
        return exps / exps.sum(-1,keepdim=True)

    def cross_entropy_forward(self, inp, targ):
         return -inp[range(targ.shape[0]), targ].mean()

    def forward(self, inp_softmax, targ):
        inp = self.softmax_forward(inp_softmax)
        return self.cross_entropy_forward(inp, targ)

    def bwd(self, out, inp, targ):
        one_hot = torch.zeros(inp.shape)
        one_hot[torch.arange(targ.shape[0]), targ] = 1

        m = targ.shape[0]
        grad = self.softmax_forward(inp)
        grad[torch.arange(m), targ] = -1
        inp.g = grad/m

    def accuracy(self, pred, yb): return (torch.argmax(pred, dim=1)==yb).float().mean().item()